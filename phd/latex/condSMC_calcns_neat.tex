\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=2cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\title{Conditional SMC with Multinomial Resampling}
\author{Suzie Brown}
\date{\today}

%\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage[round, sort&compress]{natbib}
\usepackage{har2nat} %%% Harvard reference style
\bibliographystyle{agsm}

\newcommand{\E}{\mathbb{E}}
\newcommand{\PR}{\mathbb{P}}
\newcommand{\V}{\operatorname{Var}}
\newcommand{\vt}[2][t]{v_{#1}^{(#2)}}
\newcommand{\vttilde}[2][t]{\tilde{v}_{#1}^{(#2)}}
\newcommand{\wt}[2][t]{w_{#1}^{(#2)}}
\newcommand{\eqdist}{\overset{d}{=}}
\newcommand{\Bin}{\operatorname{Bin}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\F}{\mathcal{F}_{t-1}}
\newcommand{\myrule}{\\ \rule{\textwidth}{1pt}}
\newtheorem{thm}{Theorem}

\begin{document}
\maketitle

In standard SMC with multinomial resampling, the marginal offspring distributions, conditioned on the filtration $\F$ generated by the previous offspring counts, are
\begin{equation*}
\vt{i} \eqdist \Bin (N, \wt{i}), \qquad i=1,\dots,N
\end{equation*}
where $\vt{i}$ is the number of offspring in generation $t+1$ of the $i$th particle in generation $t$, $N$ is the number of particles and $\wt{i}$ is the weight associated with the $i$th particle in generation $t$.

In conditional SMC we condition on the immortal line surviving each resampling step. By exchangeability we can set without loss of generality that the immortal line consists of particle 1 in each generation. At each resampling step, particle 1 must therefore choose particle 1 as its parent, while the remaining $N-1$ offspring are assigned multinomially to the $N$ possible parents. The marginal offspring distributions are then
\begin{align*}
& \vttilde{1} \eqdist 1 + \Bin(N-1, \wt{1}) \\
& \vttilde{i} \eqdist \Bin(N-1, \wt{i}), \qquad i=2,\dots,N
\end{align*}
where we write $\vttilde{i}$ to distinguish from $\vt{i}$. Similarly, any other quantities written with tilde are the conditional SMC analogues of the corresponding untilded quantities.
\myrule

The coalescence rate is given by
\begin{equation*}
c_N(t) := \frac{1}{(N)_2} \sum_{i=1}^{N} \E\left[ (\vt{i})_2 \right].
\end{equation*}
In the following, all expectations are conditional on $\F$ but the conditioning is omitted for notational brevity.
We will make extensive use of the formula for factorial moments of the multinomial distribution given in \citet{mosimann1962}.\\
In the standard case we have
\begin{equation*}
\E[c_N(t)] = \frac{1}{(N)_2} \sum_{i=1}^{N} (N)_2 \E\left[(\wt{i})^2\right] = \sum_{i=1}^{N} \E\left[(\wt{i})^2\right].
\end{equation*}
The corresponding quantity in the conditional case is
\begin{align*}
\E[\tilde{c}_N(t)] &= \frac{1}{(N)_2} \sum_{i=1}^{N} \E\left[ (\vttilde{i})_2 \right] \\
&= \frac{1}{(N)_2} \E\left[ (\vttilde{1})^2 - \vttilde{1} \right] + \frac{1}{(N)_2}\sum_{i=2}^{N} \E\left[ (\vttilde{i})^2 - \vttilde{i} \right] \\
&= \frac{1}{(N)_2}\left[ (N-1)(N-2)\E[(\wt{1})^2] + 2(N-1)\E[\wt{1}] \right] + \frac{1}{(N)_2} \sum_{i=2}^{N} (N-1)(N-2)\E[(\wt{i})^2] \\
&= \frac{1}{(N)_2} \sum_{i=1}^{N} (N-1)(N-2)\E[(\wt{i})^2] + \frac{1}{(N)_2} 2(N-1)\E[\wt{1}] \\
&= \frac{N-2}{N} \E[c_N(t)] + \frac{2}{N} \E[\wt{1}].
\end{align*}
\myrule

The rate of super-binary mergers is bounded above by
\begin{equation*}
D_N(t) := \frac{1}{N(N)_2} \sum_{i=1}^N (\vt{i})_2 \left( \vt{i} + \frac{1}{N} \sum_{j\neq i} (\vt{j})^2 \right).
\end{equation*}
We then separate the terms involving particle 1 (the decomposition is the same for $\tilde{D}_N(t)$):
\begin{align*}
D_N(t) &= \frac{1}{N(N)_2} (\vt{1})_2 \left(\vt{1} + \frac{1}{N} \sum_{j\neq 1} (\vt{j})^2 \right) \\
&\qquad+ \frac{1}{N(N)_2} \sum_{i\neq 1} (\vt{i})_2 \left( \vt{i} + \frac{1}{N}(\vt{1})^2 + \frac{1}{N} \sum_{1\neq j\neq i} (\vt{j})^2 \right)\\
&= \frac{1}{N(N)_2} \left( (\vt{1})^3 - (\vt{1})^2 \right) 
+ \frac{1}{N(N)_2}\sum_{i\neq 1} \left( \frac{1}{N} (\vt{1})^2(\vt{i})^2 - \frac{1}{N}\vt{1}(\vt{i})^2 \right)\\
&\qquad+ \frac{1}{N(N)_2}\sum_{i\neq 1} \left( (\vt{i})^3 - (\vt{i})^2 + \frac{1}{N}(\vt{i})^2(\vt{1})^2 - \frac{1}{N}(\vt{1})^2\vt{i} \right) \\
&\qquad+ \frac{1}{N^2(N)_2}\sum_{i\neq 1}\sum_{1\neq j \neq i} \left( (\vt{i})^2(\vt{j})^2 - \vt{i}(\vt{j})^2 \right).
\end{align*}
After a lot of algebra, we find
\begin{align*}
\E[D_N(t)] &= \frac{1}{N}\sum_{i=1}^N \left( (N-2)\E[(\wt{i})^3] + 2\E[(\wt{i})^2] + \frac{3}{N-1}\E[\wt{i}] \right) \\
&\qquad + \frac{N-2}{N^2} \sum_{i=1}^N\sum_{j\neq i} \left( (N-3) \E[(\wt{i})^2(\wt{j})^2] +  \E[(\wt{i})^2\wt{j}]  \right)
\end{align*}
and correspondingly
\begin{align*}
\E[\tilde{D}_N(t)] &= \frac{N-2}{N^2} \sum_{i=1}^N \left( (N-3)\E[(\wt{i})^3] + \left( 2 + \frac{1}{N}\right) \E[(\wt{i})^2] + \frac{3}{N-2}\E[\wt{i}] \right) \\
&\qquad + \frac{(N-2)(N-3)}{N^3} \sum_{i=1}^N\sum_{j\neq i} \left( (N-4)\E[(\wt{i})^2(\wt{j})^2] +  \E[(\wt{i})^2\wt{j}] \right) \\
&\qquad + \frac{1}{N^2} \E[\wt{1}] + \left(3 - \frac{1}{N} \right)\frac{(N-2)}{N^2}\E[(\wt{1})^2] +
\frac{2(N-2)}{N^3} \sum_{i=2}^N \E[\wt{1}\wt{i}] + \frac{4(N-2)(N-3)}{N^3} \sum_{i=2}^N \E[\wt{1}(\wt{i})^2] \\
&\leq \E[D_N(t)] + \frac{1}{N^3}\sum_{i=1}^N \E[(\wt{i})^2] + \frac{1}{N^2} \E[\wt{1}] + \frac{3}{N}\E[(\wt{1})^2] +
\frac{2}{N^2} \sum_{i=2}^N \E[\wt{1}\wt{i}] + \frac{4}{N} \sum_{i=2}^N \E[\wt{1}(\wt{i})^2].
\end{align*}
\myrule

We also require a bound on the squared coalescence rate, obtained by a similar separation of terms involving particle 1.
\begin{align*}
\E[c_N(t)^2] &= \frac{1}{(N)_2^2} \E\left[ \left( \sum_{i=1}^N (\vt{i})_2\right)^2\right] \\
&= \frac{1}{(N)_2^2} \left\{ \sum_{i=1}^N \E[(\vt{i})_2^2] + \sum_{i=1}^N\sum_{j\neq i} \E[(\vt{i})_2(\vt{j})_2] \right\} \\
&= \frac{1}{(N)_2^2} \left\{ (N)_4 \sum_{i=1}^N \E[(\wt{i})^4] + 4(N)_3 \sum_{i=1}^N \E[(\wt{i})^3] + 2(N)_2 \sum_{i=1}^N \E[(\wt{i})^2] \right\} \\
&\qquad + \frac{1}{(N)_2^2}(N)_4 \sum_{i=1}^N\sum_{j\neq i} \E[(\wt{i})^2(\wt{j})^2]
\end{align*}
and
\begin{align*}
\E[\tilde{c}_N(t)^2] 
&= \frac{1}{(N)_2^2} \left\{ \sum_{i=1}^N \E[(\vttilde{i})_2^2] 
+ \sum_{i=1}^N\sum_{j\neq i} \E[(\vttilde{i})_2(\vttilde{j})_2] \right\} \\
&= \frac{1}{(N)_2^2} \left\{ \sum_{i=2}^N \E[(\vttilde{i})_2^2] +\E[(\vttilde{1})_2^2] 
+ \sum_{i=2}^N\sum_{1\neq j\neq i} \E[(\vttilde{i})_2(\vttilde{j})_2] + 2\sum_{i=1}^N \E[(\vttilde{1})_2(\vttilde{i})_2] \right\} \\
&= \frac{1}{(N)_2^2} \left\{ (N-1)_4 \sum_{i=1}^N \E[(\wt{i})^4] + 4(N-1)_3 \sum_{i=1}^N \E[(\wt{i})^3] + 2(N-1)_2 \sum_{i=1}^N \E[(\wt{i})^2] \right\} \\
&\qquad + \frac{1}{(N)_2^2} \left\{ (N-1)_4 \sum_{i=1}^N\sum_{j\neq i} \E[(\wt{i})^2(\wt{j})^2] 
+ 4(N-1)_3 \E[(\wt{1})^3] + 12(N-1)_2\E[(\wt{1})^2] + 4(N-1)\E[\wt{1}] \right\} \\
&\qquad +\frac{1}{(N)_2^2} \left\{ 2(N-1)_3 \sum_{i=2}^N \E[\wt{1}(\wt{i})^2] \right\} \\
&\leq  \E[c_N(t)^2] + \frac{1}{(N)_2^2} \left\{4(N-1)_3 \E[(\wt{1})^3] + 12(N-1)_2\E[(\wt{1})^2] + 4(N-1)\E[\wt{1}]
+ 2(N-1)_3 \sum_{i=2}^N \E[\wt{1}(\wt{i})^2] \right\}
\end{align*}
\myrule

We can modify the proof of \citet[Lemma 3]{koskela2018} by replacing the untilded quantities with these modified expressions.
We still assume the conditions (18) and (19) of the lemma, which control the weights and allow us to find the following asymptotic expressions:
\begin{align*}
& \E[\tilde{c}_N(t)] \leq \E[c_N(t)] + O(N^{-2}) \\
& \E[\tilde{c}_N(t)^2] \leq \E[c_N(t)^2] + O(N^{-3}) \\
& \E[\tilde{D}_N(t)] \leq \E[D_N(t)] +O(N^{-3})
\end{align*}
Therefore we get the upper bound
\begin{align*}
\E[\tilde{c}_N(t)] &= \frac{N-2}{N} \E[c_N(t)] + \frac{2}{N} \E[\wt{1}] \\
& \leq \E[c_N(t)] + \frac{2}{N} \E[\wt{1}] \\
&\leq \frac{a^4}{N\varepsilon^4} +  \frac{2}{N} \E[\wt{1}] \\
&= \frac{a^4}{N\varepsilon^4} + O(N^{-2})
\end{align*}
and lower bound
\begin{align*}
\E[\tilde{c}_N(t)] &= \frac{N-2}{N} \E[c_N(t)] + \frac{2}{N} \E[\wt{1}] \\
& \geq  \frac{N-2}{N}\frac{\varepsilon^4}{Na^4} + O(N^{-2}) \\
& = \frac{\varepsilon^4}{Na^4} - \frac{2\varepsilon^4}{N^2a^4} + O(N^{-2}) \\
& = \frac{\varepsilon^4}{Na^4} + O(N^{-2})
\end{align*}
as in \citet{koskela2018}, with the addition of an error term.
Using that (22) in the lemma implies $\E[c_N(t)] = O(N^{-1})$, we also have the reverse inequality
\begin{equation*}
\E[\tilde{c}_N(t)] = \E[c_N(t)] - \frac{2}{N}\E[c_N(t)] + O(N^{-2}) \geq \E[c_N(t)] + O(N^{-2}).
\end{equation*}
This allows us to deduce the following upper bounds:
\begin{align*}
\E[\tilde{D}_N(t)] &\leq \E[D_N(t)] + O(N^{-3}) \\
&\leq \frac{C}{N} \E[c_N(t)] + O(N^{-3}) \\
&= \frac{C}{N} \E[\tilde{c}_N(t)] + O(N^{-3})
\end{align*}
and
\begin{align*}
\E[\tilde{c}_N(t)^2] &\leq \E[c_N(t)^2] + O(N^{-3}) \\
&\leq \frac{C}{N} \E[c_N(t)] + O(N^{-3}) \\
&= \frac{C}{N} \E[\tilde{c}_N(t)] + O(N^{-3})
\end{align*}
which correspond to (20) and (21) respectively in the statement of the lemma, again with additional error terms. $C$ indicates an arbitrary constant which may change from one line to the next.

We conclude an analogous result to \citet[Lemma 3]{koskela2018} for conditional SMC, involving the tilded quantities and with the addition of some vanishing errors that should not affect the further analysis.

\bibliography{smc.bib}
\end{document}