\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=2cm]{geometry}

\usepackage{amsmath}

% custom header/footer
\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\fancyhf{}
\rfoot{\textsf{\thepage}}
\lfoot{\textsf{Suzie Brown}}

\title{Possible future research topics}
\author{Suzie Brown}
\date{1 July 2021}

\begin{document}
\maketitle
\thispagestyle{fancy}

\section*{Extending the theoretical results}
\begin{itemize}
\item Relax condition that potentials are bounded below (e.g.\ see Capp\'e, Moulines, Ryd\'en ``Inference in HMMs'', Chapter 9)
\item Show that the condition of BJJK Theorem 1 is necessary as well as sufficient (e.g.\ see M\"ohle \& Sagitov 1998, 2001, 2003?)
\item What is the effect of adaptive resampling?
\item Corollaries also for residual-multinomial and residual-star resampling (although it might be a negative result in the case of residual-star).
\item Rates of convergence (modifying more of M\"ohle's work).
\item Could badly-behaved potentials produce a non-Kingman Lambda-coalescent? For example, use for potentials some heavy-tailed fitnesses like in Schweinsberg model. (This is probably not interesting from an SMC point of view.)
\item A way to estimate coalescent rates / time scale a priori, for some specific tractable class of models, say.
\item Do our results apply to cloning models, or other continuous-time models?
\item See if Jacob \& Rubenthaler's (``path storage in the PF'') brute-force technique can be adapted for use in Conditional SMC, to UB tree height
\item Does CSMC with an ``unlikely'' immortal line converge to a structured coalescent?
\end{itemize}

\section*{Resampling}
\begin{itemize}
\item Derive expressions for the one-step Monte Carlo variance with SSP resampling and residual-systematic resampling, (and systematic resampling?). Is there another ordering result there?
\item Compare residual-systematic vs. systematic resampling: are they equivalent? What if the weights are sorted? Maybe start by coding up an exploratory experiment.
\item Explore more generally the effect of pre-sorting the weights. How does this link with results of Gerber Chopin Whiteley (where they sort by particle position)?
\item Compare theoretical computation/storage costs and parallelisability between the different resampling schemes.
\item Can the comparison of time-scales for different schemes be wrangled into/ related to a direct comparison of the variances? Is this even useful?
\item (See marked notebook 3 page circa Jun/Jul 2020 for some more thoughts/ideas.)
\item The difference $\Delta_i$, as defined in \texttt{phd/latex/randomised\_rounding/randomised\_rounding.pdf}, seems to tend to a quadratic in the weight as $N\to\infty$. Prove it?
\item (Just for fun:) in stochastic rounding, how many possible ways are there to assign the offspring counts? Consider that each of the N counts takes one of two possible values, but this will overcount a lot because we are also constrained by offspring counts summing to N. It's $\binom{N}{R}$ isn't it? Where $R$ as in residual resampling. Give each parent its minimum number of offspring, and you'll be left with $R$ unassigned offspring that have to be given to $R$ distinct parents among $N$. $\binom{N}{R}$ is maximised if $R\simeq N/2$, in which case $\binom{N}{R}\simeq ??$.
\item (Conjecture:) pre-sorting of weights reduces resampling variance, but increases the coalescence rate. Intuition: when weights are sorted, small weights that sum to less than 1/N are grouped together so only one of them can have a child. (It may be that this effect is entirely cancelled by the reduction in variance elsewhere.)
\end{itemize}

\section*{Simulation experiments}
\begin{itemize}
\item Now we have weak convergence, redo similar experiments to those at end of KJJS, but without having to fudge it.
\item Code up all the different sampling schemes (and sorted/unsorted variants etc.) and come up with some useful (if only illustrative) experiments comparing them. (I started doing this by plotting $\tau_N(t)$ against $t$, and got some nice results.)
\item Decide which functions would be most interesting to illustrate the performance of resampling schemes, say using ternary diagrams. (See marked notebook 3 page at 8/7/20.)
\item Explore pre-asymptotic behaviour of CSMC for example. I did some work on this previously.
\item Make edits to my ternary plot ``library'' (see notebook 3 marked page at 8/7/20).
\end{itemize}




\end{document}