\documentclass[fleqn]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\title{Conditional SMC with Multinomial Resampling}
\author{Suzie Brown}
\date{\today}

%\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage[round, sort&compress]{natbib}
\usepackage{har2nat} %%% Harvard reference style
\bibliographystyle{agsm}

\newcommand{\E}{\mathbb{E}}
\newcommand{\PR}{\mathbb{P}}
\newcommand{\V}{\operatorname{Var}}
\newcommand{\vt}[2][t]{v_{#1}^{(#2)}}
\newcommand{\vttilde}[2][t]{\tilde{v}_{#1}^{(#2)}}
\newcommand{\wt}[2][t]{w_{#1}^{(#2)}}
\newcommand{\eqdist}{\overset{d}{=}}
\newcommand{\Bin}{\operatorname{Bin}}
\newcommand{\Mn}{\operatorname{Multinomial}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\F}{\mathcal{F}_{t-1}}
\newcommand{\Ntoinfty}{\overset{N\to\infty}{\longrightarrow}}
\newcommand{\myrule}{\\ \rule{\textwidth}{1pt}}
\newtheorem{thm}{Theorem}

\begin{document}
\maketitle

In standard SMC with multinomial resampling, the marginal offspring distributions, conditioned on the filtration $\F$ generated by the previous offspring counts, are
\begin{equation*}
\vt{i} \eqdist \Bin (N, \wt{i}), \qquad i=1,\dots,N
\end{equation*}
where $\vt{i}$ is the number of offspring in generation $t+1$ of the $i$th particle in generation $t$, $N$ is the number of particles and $\wt{i}$ is the weight associated with the $i$th particle in generation $t$.

In conditional SMC we condition on the immortal line surviving each resampling step. By exchangeability we can set without loss of generality that the immortal line consists of particle 1 in each generation. At each resampling step, particle 1 must therefore choose particle 1 as its parent, while the remaining $N-1$ offspring are assigned multinomially to the $N$ possible parents. The marginal offspring distributions are then
\begin{align*}
& \vttilde{1} \eqdist 1 + \Bin(N-1, \wt{1}) \\
& \vttilde{i} \eqdist \Bin(N-1, \wt{i}), \qquad i=2,\dots,N
\end{align*}
where we write $\vttilde{i}$ to distinguish from $\vt{i}$. Similarly, any other quantities written with tilde are the conditional SMC analogues of the corresponding untilded quantities.
\myrule

In the following we will make extensive use of the formula for factorial moments of the multinomial distribution given in \citet[p.67]{mosimann1962}, which has the convenient form
\begin{equation*}
\E[(X_i)_a(X_j)_b] = (n)_{a+b}\, p_i^a p_j^b
\end{equation*}
where $(X_1,\dots X_k) \sim \Mn(n, \mathbf{p})$. (By convention $(X)_0=1$.)
To take advantage of this convenient expression we will convert any ordinary moments appearing in the calculations into factorial moments, using the identities summarised in Table \ref{tab:fact_powers}.
%Let us denote the random variables
%\begin{align*}
%& X_1,\dots, X_N \sim \Mn(N, \mathbf{w}) \\
%& \tilde{X}_1,\dots, \tilde{X}_N \sim \Mn(N-1, \mathbf{w}) \\
%& \mathbf{w} := (\wt{1},\dots, \wt{N})
%\end{align*}
\myrule

\begin{table}
\centering
\begin{tabular}{r c l}
\hline
$x$ & $=$ & $(x)_1$ \\
$x^2$ & $=$ & $(x)_2 + (x)_1$ \\
$x^3$ & $=$ & $(x)_3 + 3(x)_2 + (x)_1$ \\
$x^4$ & $=$ & $(x)_4 + 6(x)_3 + 7(x)_2 + (x)_1$ \\
\hline
$xy$ & $=$ & $(x)_1(y)_1$ \\
$x^2y$ & $=$ & $(x)_2(y)_1 + (x)_1(y)_1$ \\
$xy^2$ & $=$ & $(x)_1(y)_2 + (x)_1(y)_1$ \\
$x^2y^2$ & $=$ & $(x)_2(y)_2 + (x)_2(y)_1 + (x)_1(y)_2 +  (x)_1(y)_1$ \\
\hline
\end{tabular}
\caption{Conversion of ordinary powers into falling factorial powers}
\label{tab:fact_powers}
\end{table}

The coalescence rate is given by
\begin{equation*}
c_N(t) := \frac{1}{(N)_2} \sum_{i=1}^{N} \E\left[ (\vt{i})_2 \right].
\end{equation*}
In the standard case we have, using the tower rule and the multinomial moments,
\begin{equation*}
\E[c_N(t) |\F] 
= \frac{1}{(N)_2} \sum_{i=1}^{N} \E\left[ \E[ (\vt{i})_2 ] |\F \right]
=\frac{1}{(N)_2} \sum_{i=1}^{N} \E\left[(N)_2 (\wt{i})^2 |\F \right] 
= \sum_{i=1}^{N} \E\left[(\wt{i})^2 |\F \right]
\end{equation*}
The corresponding quantity in the conditional case is
\begin{equation*}
\tilde{c}_N(t) = \frac{1}{(N)_2} \sum_{i=1}^{N} \E\left[ (\vttilde{i})_2 \right] 
= \frac{1}{(N)_2} \E\left[ (\vttilde{1})_2 \right] + \frac{1}{(N)_2}\sum_{i=2}^{N} \E\left[ (\vttilde{i})_2 \right] \\
\end{equation*}
Then applying the tower rule and factorial moments as before we obtain
\begin{align*}
\E[\tilde{c}_N(t) |\F] &= \frac{1}{(N)_2} \left\{ (N-1)_2\E[(\wt{1})^2 |\F] + 2(N-1)\E[\wt{1} |\F] + \sum_{i=2}^{N} (N-1)_2\E[(\wt{i})^2 |\F] \right\}\\
&= \frac{(N-1)_2}{(N)_2} \sum_{i=1}^{N} \E[(\wt{i})^2 |\F] + \frac{2(N-1)}{(N)_2} \E[\wt{1} |\F] \\
&= \frac{N-2}{N} \E[c_N(t) |\F] + \frac{2}{N} \E[\wt{1} |\F]
\end{align*}
\myrule

The rate of super-binary mergers is bounded above by
\begin{align*}
D_N(t) &:= \frac{1}{N(N)_2} \sum_{i=1}^N (\vt{i})_2 \left( \vt{i} + \frac{1}{N} \sum_{j\neq i} (\vt{j})^2 \right) \\
&= \frac{1}{N(N)_2} \sum_{i=1}^N \left\{ (\vt{i})_2\vt{i} + \frac{1}{N}\sum_{j\neq i} (\vt{i})_2(\vt{j})^2 \right\} \\
&= \frac{1}{N(N)_2} \sum_{i=1}^N \left\{ (\vt{i})_3 + 2(\vt{i})_2 + \frac{1}{N}\sum_{j\neq i} \left( (\vt{i})_2(\vt{j})_2 + (\vt{i})_2\vt{j} \right) \right\}
\end{align*}
We therefore find
\begin{align*}
\E[D_N(t) |\F] &= \frac{1}{N(N)_2} \sum_{i=1}^N \left\{ (N)_3\E[(\wt{i})^3 |\F] + 2(N)_2\E[(\wt{i})^2 |\F] \right\} \\
&\qquad + \frac{1}{N^2(N)_2} \sum_{i=1}^N \sum_{j\neq i} \left\{ (N)_4\E[(\wt{i})^2(\wt{j})^2 |\F] + (N)_3\E[(\wt{i})^2\wt{j} |\F] \right\}
\end{align*}
For the conditional case, we separate the terms involving particle 1:
\begin{align*}
\tilde{D}_N(t) &= \frac{1}{N(N)_2} (\vttilde{1})_2 \left(\vttilde{1} + \frac{1}{N} \sum_{j\neq 1} (\vttilde{j})^2 \right)
+ \frac{1}{N(N)_2} \sum_{i\neq 1} (\vttilde{i})_2 \left( \vttilde{i} + \frac{1}{N}(\vttilde{1})^2 + \frac{1}{N} \sum_{1\neq j\neq i} (\vttilde{j})^2 \right)\\
&= \frac{1}{N(N)_2} \left\{ (\vttilde{1})_2\vt{1} + \frac{1}{N}\sum_{j\neq 1} (\vttilde{1})_2(\vttilde{j})^2 + \frac{1}{N}\sum_{i\neq 1} (\vttilde{i})_2(\vttilde{1})^2 \right\} \\
&\qquad + \frac{1}{N(N)_2} \sum_{i\neq 1} \left\{ (\vttilde{i})_2\vttilde{i} + \frac{1}{N}\sum_{1\neq j\neq i} (\vttilde{i})_2(\vttilde{j})^2 \right\} \\
&= \frac{1}{N(N)_2} \left\{ (\vttilde{1})_2\vttilde{1} + \frac{1}{N}\sum_{i\neq 1} \left( (\vttilde{1})_2(\vttilde{i})^2 + (\vttilde{i})_2(\vttilde{1})^2 \right) \right\} \\
&\qquad + \frac{1}{N(N)_2} \sum_{i\neq 1} \left\{ (\vttilde{i})_3 + 2(\vttilde{i})_2 + \frac{1}{N}\sum_{1\neq j\neq i} \left( (\vttilde{i})_2(\vttilde{j})_2 + (\vttilde{i})_2\vttilde{j} \right) \right\}
\end{align*}
and we find the expectation:
\begin{align*}
\E[\tilde{D}_N(t) |\F] &= \frac{1}{N(N)_2} \left\{ (N-1)_3\E[(\wt{1})^3 |\F] + 5(N-1)_2\E[(\wt{1})^2 |\F] +  4(N-1)\E[\wt{1} |\F] \right\} \\
&\qquad + \frac{1}{N^2(N)_2} \sum_{i=2}^N \left\{ 2(N-1)_4\E[(\wt{1})^2(\wt{i})^2 |\F] + (N-1)_3 \E[(\wt{1})^2\wt{i} |\F] \right.\\
&\qquad\qquad + 5(N-1)_3\E[\wt{1}(\wt{i})^2 |\F] 
 \left. +2(N-1)_2\E[\wt{1}\wt{i} |\F] + (N-1)_2\E[(\wt{i})^2 |\F] \right\} \\
&\qquad + \frac{1}{N(N)_2} \sum_{i=2}^N \left\{ 
(N-1)_3\E[(\wt{i})^3 |\F] + 2(N-1)_2\E[(\wt{i})^2 |\F] \right\} \\
&\qquad + \frac{1}{N^2(N)_2} \sum_{i=2}^N \sum_{1\neq j\neq i} \left\{ (N-1)_4\E[(\wt{i})^2(\wt{j})^2 |\F] + (N-1)_3\E[(\wt{i})^2\wt{j} |\F] \right\} \\
&= \frac{1}{N(N)_2} \sum_{i=1}^N \left\{ 
(N-1)_3\E[(\wt{i})^3 |\F] + 2(N-1)_2\E[(\wt{i})^2 |\F] \right\} \\
&\qquad + \frac{1}{N^2(N)_2} \sum_{i=1}^N \sum_{j\neq i} \left\{ (N-1)_4\E[(\wt{i})^2(\wt{j})^2 |\F] + (N-1)_3\E[(\wt{i})^2\wt{j} |\F] \right\} \\
&\qquad + \frac{1}{N(N)_2}\left\{ 3(N-1)_2\E[(\wt{1})^2 |\F] +  4(N-1)\E[\wt{1} |\F] \right\} \\
&\qquad+ \frac{1}{N^2(N)_2}\sum_{i=2}^N \left\{ 4(N-1)_3\E[\wt{1}(\wt{i})^2 |\F]
+2(N-1)_2\E[\wt{1}\wt{i} |\F] + (N-1)_2\E[(\wt{i})^2 |\F] \right\} \\
&\leq \E[D_N(t) |\F] + \frac{1}{N(N)_2} \left\{3(N-1)_2\E[(\wt{1})^2 |\F] +  4(N-1)\E[\wt{1} |\F] \right\} \\
&\qquad+ \frac{1}{N^2(N)_2}\sum_{i=2}^N \left\{ 4(N-1)_3\E[\wt{1}(\wt{i})^2 |\F] +2(N-1)_2\E[\wt{1}\wt{i} |\F] + (N-1)_2\E[(\wt{i})^2 |\F] \right\} \\
&\leq \E[D_N(t) |\F] + \frac{3}{N} \E[(\wt{1})^2 |\F] +  \frac{4}{N^2}\E[\wt{1} |\F] \\
&\qquad+ \frac{4}{N}\sum_{i=2}^N \E[\wt{1}(\wt{i})^2 |\F] +\frac{2}{N^2}\sum_{i=2}^N \E[\wt{1}\wt{i} |\F] + \frac{1}{N^2} \sum_{i=2}^N \E[(\wt{i})^2 |\F] 
\end{align*}
For the second equality we recombine the particle 1 terms into the sum, and the upper bounds follow from bounding e.g.\ $N-1$ by $N$.
\myrule

We also require a bound on the squared coalescence rate, expanded thus in \citet{koskela2018}:
\begin{align*}
c_N(t)^2 &= \frac{1}{(N)_2^2} \left( \sum_{i=1}^N (\vt{i})_2\right)^2 = \frac{1}{(N)_2^2} \left\{ \sum_{i=1}^N \E[(\vt{i})_2^2] + \sum_{i=1}^N\sum_{j\neq i} \E[(\vt{i})_2(\vt{j})_2] \right\}
\end{align*}
Here we use a more explicit expansion for direct comparison with the conditional version.
\begin{align*}
\E[c_N(t)^2 |\F] &= \frac{1}{(N)_2^2} \left\{ (N)_4 \sum_{i=1}^N \E[(\wt{i})^4 |\F] + 4(N)_3 \sum_{i=1}^N \E[(\wt{i})^3 |\F] + 2(N)_2 \sum_{i=1}^N \E[(\wt{i})^2 |\F] \right\} \\
&\qquad + \frac{1}{(N)_2^2}(N)_4 \sum_{i=1}^N\sum_{j\neq i} \E[(\wt{i})^2(\wt{j})^2 |\F] \\
&= \frac{1}{(N)_2} \left\{ (N-2)_2 \sum_{i=1}^N \E[(\wt{i})^4 |\F] + 4(N-2) \sum_{i=1}^N \E[(\wt{i})^3 |\F] + 2 \sum_{i=1}^N \E[(\wt{i})^2 |\F] \right. \\
&\hspace{230pt}+ \left. (N-2)_2 \sum_{i=1}^N\sum_{j\neq i} \E[(\wt{i})^2(\wt{j})^2 |\F] \right\}
\end{align*}
In the conditional version, the calculations follow the same structure as those for $\tilde{D}_N(t)$, first separating the terms in particle 1:
\begin{align*}
\tilde{c}_N(t)^2 &= \frac{1}{(N)_2^2} \left\{ \sum_{i=1}^N \E[(\vttilde{i})_2^2] 
+ \sum_{i=1}^N\sum_{j\neq i} \E[(\vttilde{i})_2(\vttilde{j})_2] \right\} \\
&= \frac{1}{(N)_2^2} \left\{ \sum_{i=2}^N \E[(\vttilde{i})_2^2] +\E[(\vttilde{1})_2^2] 
+ \sum_{i=2}^N\sum_{1\neq j\neq i} \E[(\vttilde{i})_2(\vttilde{j})_2] + 2\sum_{i=1}^N \E[(\vttilde{1})_2(\vttilde{i})_2] \right\} \\
\end{align*}
And we find the following expectation, using the same techniques as before:
\begin{align*}
\E[\tilde{c}_N(t)^2 |\F] &= \frac{1}{(N)_2^2} \left\{ (N-1)_4 \sum_{i=2}^N \E[(\wt{i})^4 |\F] + 4(N-1)_3 \sum_{i=2}^N \E[(\wt{i})^3 |\F] + 2(N-1)_2 \sum_{i=2}^N \E[(\wt{i})^2 |\F] \right\}  \\
&\qquad + \frac{1}{(N)_2^2} \left\{ (N-1)_4\E[(\wt{1})^4 |\F] + 8(N-1)_3\E[(\wt{1})^3 |\F] + 14(N-1)_2\E[(\wt{1})^2 |\F] \right\} \\
&\qquad +  \frac{1}{(N)_2^2}4(N-1)\E[\wt{1} |\F]
+ \frac{1}{(N)_2^2}  (N-1)_4 \sum_{i=2}^N \sum_{1\neq j \neq i} \E[(\wt{i})^2(\wt{j})^2 |\F] \\
&\qquad +\frac{2}{(N)_2^2} \sum_{i=2}^N \left( (N-1)_4 \E[(\wt{1})^2(\wt{i})^2 |\F] + 2(N-1)_3 \E[\wt{1}(\wt{i})^2 |\F] \right) \\
&= \frac{1}{(N)_2^2} \left\{ (N-1)_4 \sum_{i=1}^N \E[(\wt{i})^4 |\F] + 4(N-1)_3 \sum_{i=1}^N \E[(\wt{i})^3 |\F] + 2(N-1)_2 \sum_{i=1}^N \E[(\wt{i})^2 |\F] \right\} \\
&\qquad + \frac{(N-1)_4}{(N)_2^2} \sum_{i=1}^N\sum_{j\neq i} \E[(\wt{i})^2(\wt{j})^2 |\F] \\
&\qquad +\frac{1}{(N)_2^2} \left\{ 4(N-1)_3 \E[(\wt{1})^3 |\F] + 12(N-1)_2\E[(\wt{1})^2 |\F] + 4(N-1)\E[\wt{1} |\F] \right\} \\
&\qquad +\frac{1}{(N)_2^2} \left\{ 4(N-1)_3 \sum_{i=2}^N \E[\wt{1}(\wt{i})^2 |\F] \right\} \\
&\leq  \E[c_N(t)^2 |\F] + \frac{1}{(N)_2^2} \left\{4(N-1)_3 \E[(\wt{1})^3 |\F] + 12(N-1)_2 \E[(\wt{1})^2 |\F] \right\}\\
&\qquad +\frac{1}{(N)_2^2} \left\{ 4(N-1)\E[\wt{1} |\F]
+ 4(N-1)_3 \sum_{i=2}^N \E[\wt{1}(\wt{i})^2 |\F] \right\} \\
&\leq \E[c_N(t)^2 |\F] + \frac{4}{N}\E[(\wt{1})^3 |\F] + \frac{12}{N^2} \E[(\wt{1})^2 |\F] + \frac{4}{N(N)_2} \E[\wt{1} |\F]\\
&\qquad + \frac{4}{N} \sum_{i=2}^N \E[\wt{1}(\wt{i})^2 |\F]
\end{align*}
\myrule

We can modify the proof of \citet[Lemma 3]{koskela2018} for conditional SMC by replacing the untilded quantities with these modified expressions.
We still assume the conditions (18) and (19) of the lemma, which control the weights. This allows us to simplify the bounds derived in the above to the following asymptotic expressions:
\begin{align*}
& \E[\tilde{c}_N(t) |\F] \leq \E[c_N(t) |\F] + O(N^{-2}) \\
& \E[\tilde{D}_N(t) |\F] \leq \E[D_N(t) |\F] +O(N^{-3}) \\
& \E[\tilde{c}_N(t)^2 |\F] \leq \E[c_N(t)^2 |\F] + O(N^{-3})
\end{align*}
Therefore we get the upper bound
\begin{align}\label{eq:cN_upper}
\E[\tilde{c}_N(t) |\F] &= \frac{N-2}{N} \E[c_N(t) |\F] + \frac{2}{N} \E[\wt{1} |\F] \notag\\
& \leq \E[c_N(t) |\F] + \frac{2}{N} \E[\wt{1} |\F] \notag\\
&\leq \frac{a^4}{N\varepsilon^4} +  \frac{2}{N} \E[\wt{1} |\F] \notag\\
&= \frac{a^4}{N\varepsilon^4} + O(N^{-2})
\end{align}
and lower bound
\begin{align}\label{eq:cN_lower}
\E[\tilde{c}_N(t) |\F] &= \frac{N-2}{N} \E[c_N(t) |\F] + \frac{2}{N} \E[\wt{1} |\F] \notag\\
& \geq  \frac{N-2}{N}\frac{\varepsilon^4}{Na^4} + O(N^{-2}) \notag\\
& = \frac{\varepsilon^4}{Na^4} - \frac{2\varepsilon^4}{N^2a^4} + O(N^{-2}) \notag\\
& = \frac{\varepsilon^4}{Na^4} + O(N^{-2})
\end{align}
as in \citet{koskela2018}, but with the addition of an error term.
Using that (22) in the lemma implies $\E[c_N(t)] = O(N^{-1})$, we also have the reverse inequality
\begin{equation*}
\E[\tilde{c}_N(t) |\F] = \E[c_N(t) |\F] - \frac{2}{N}\E[c_N(t) |\F] + O(N^{-2}) \geq \E[c_N(t) |\F] + O(N^{-2})
\end{equation*}
which allows us to deduce the following upper bounds:
\begin{align}\label{eq:DN_upper}
\E[\tilde{D}_N(t) |\F] &\leq \E[D_N(t) |\F] + O(N^{-3}) \notag\\
&\leq \frac{C}{N} \E[c_N(t) |\F] + O(N^{-3}) \notag\\
&= \frac{C}{N} \E[\tilde{c}_N(t) |\F] + O(N^{-3})
\end{align}
and
\begin{align}\label{eq:cN2_upper}
\E[\tilde{c}_N(t)^2 |\F] &\leq \E[c_N(t)^2 |\F] + O(N^{-3}) \notag\\
&\leq \frac{C}{N} \E[c_N(t) |\F] + O(N^{-3}) \notag\\
&= \frac{C}{N} \E[\tilde{c}_N(t) |\F] + O(N^{-3})
\end{align}
These correspond to (20) and (21) respectively in the statement of the lemma, again with additional error terms. $C$ indicates an arbitrary constant which may change from one line to the next.

We conclude an analogous result to \citet[Lemma 3]{koskela2018} for conditional SMC, involving the tilded quantities and with the addition of some vanishing errors that should not affect the further analysis.
\myrule

The proof of Lemma 2 in \citet{koskela2018} applies to a general class of functions which includes our tilded ones, so that result can be applied in the following without modification.

At this point we define the modified time-rescaling.
\begin{equation*}
\tilde{\tau}_N(t) := \min\left\{ s\geq 1 : \sum_{r=1}^s \tilde{c}_N(r) \geq t \right\}
\end{equation*}
This is a generalised inverse of $\tilde{c}_N(t)$, so in particular we have the property
\begin{equation}\label{eq:sum_tau_cN_upper}
\sum_{r=\tilde{\tau}_N(s)}^{\tilde{\tau}_N(t)} \tilde{c}_N(r) \leq t-s +1 .
\end{equation}
Next we must check that conditional SMC with multinomial resampling satisfies the conditions of Theorem 1 from \citet{koskela2018}:
\begin{description}
\item{\textbf{Standing assumption}} holds by exchangeability of multinomial resampling?
\item{\textbf{(4)}} Using \eqref{eq:cN_upper} and applying the tower rule, we find
\begin{equation*}
\E[\tilde{c}_N(t)] = \E[\E[\tilde{c}_N(t) |\F]] \leq \frac{a^4}{N\varepsilon^4} + O(N^{-2}) \Ntoinfty 0
\end{equation*}
\item{\textbf{(5)}} Using \citet[Lemma 2]{koskela2018} along with \eqref{eq:cN2_upper} and \eqref{eq:sum_tau_cN_upper},
\begin{align*}
\E\left[ \sum_{r=\tilde{\tau}_N(s)}^{\tilde{\tau}_N(t)} \tilde{c}_N(r)^2 \right] 
&= \E\left[ \sum_{r=\tilde{\tau}_N(s)}^{\tilde{\tau}_N(t)} \E[\tilde{c}_N(r) |\F] \right]
\leq \frac{C}{N}  \E\left[ \sum_{r=\tilde{\tau}_N(s)}^{\tilde{\tau}_N(t)} \E[\tilde{c}_N(t) |\F] + O(N^{-3}) \right] \\
&= \frac{C}{N}  \E\left[ \sum_{r=\tilde{\tau}_N(s)}^{\tilde{\tau}_N(t)} \E[\tilde{c}_N(t) |\F] \right] + O(N^{-3})
= \frac{C}{N}  \E\left[ \sum_{r=\tilde{\tau}_N(s)}^{\tilde{\tau}_N(t)} \tilde{c}_N(r) \right] + O(N^{-3}) \\
&\leq \frac{C}{N}(t-s+1) + O(N^{-3}) \Ntoinfty 0
\end{align*}
\item{\textbf{(3)}} The above caluclation replacing \eqref{eq:cN2_upper} with \eqref{eq:DN_upper} yields
\begin{equation*}
\E\left[ \sum_{r=\tilde{\tau}_N(s)}^{\tilde{\tau}_N(t)} \tilde{D}_N(r) \right] \Ntoinfty 0
\end{equation*}
\item{\textbf{(6)}} Using \eqref{eq:cN_lower}, \eqref{eq:sum_tau_cN_upper} and \citet[Lemma 2]{koskela2018},
\begin{align*}
\E[\tau_N(t) - \tau_N(s)] 
&= \E\left[ \sum_{r=\tilde{\tau}_N(s)}^{\tilde{\tau}_N(t)} 1 \right] 
= \E\left[ \sum_{r=\tilde{\tau}_N(s)}^{\tilde{\tau}_N(t)} \frac{\E[\tilde{c}_N(r) |\F]}{\E[\tilde{c}_N(r) |\F]} \right] 
= \frac{1}{\E[\tilde{c}_N(r) |\F]} \E\left[ \sum_{r=\tilde{\tau}_N(s)}^{\tilde{\tau}_N(t)} \E[\tilde{c}_N(r) |\F] \right] \\
&= \frac{1}{\E[\tilde{c}_N(r) |\F]} \E\left[ \sum_{r=\tilde{\tau}_N(s)}^{\tilde{\tau}_N(t)} \tilde{c}_N(r) \right]
\leq \frac{t-s+1}{\frac{\varepsilon^4}{Na^4	}+O(N^{-2})} = \frac{(t-s+1)a^4 N}{\varepsilon^4 + O(N^{-3})} 
\sim C_{t,s} N.
\end{align*}
\end{description}

\bibliography{smc.bib}
\end{document}