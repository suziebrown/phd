\documentclass[fleqn]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\title{Conditional SMC with Multinomial Resampling}
\author{Suzie Brown}
\date{\today}

%\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage[round, sort&compress]{natbib}
\usepackage{har2nat} %%% Harvard reference style
\bibliographystyle{agsm}

\newcommand{\E}{\mathbb{E}}
\newcommand{\PR}{\mathbb{P}}
\newcommand{\V}{\operatorname{Var}}
\newcommand{\vt}[2][t]{v_{#1}^{(#2)}}
\newcommand{\vttilde}[2][t]{\tilde{v}_{#1}^{(#2)}}
\newcommand{\wt}[2][t]{w_{#1}^{(#2)}}
\newcommand{\eqdist}{\overset{d}{=}}
\newcommand{\Bin}{\operatorname{Bin}}
\newcommand{\Mn}{\operatorname{Multinomial}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\F}{\mathcal{F}_{t-1}}
\newcommand{\myrule}{\\ \rule{\textwidth}{1pt}}
\newtheorem{thm}{Theorem}

\begin{document}
\maketitle

In standard SMC with multinomial resampling, the marginal offspring distributions, conditioned on the filtration $\F$ generated by the previous offspring counts, are
\begin{equation*}
\vt{i} \eqdist \Bin (N, \wt{i}), \qquad i=1,\dots,N
\end{equation*}
where $\vt{i}$ is the number of offspring in generation $t+1$ of the $i$th particle in generation $t$, $N$ is the number of particles and $\wt{i}$ is the weight associated with the $i$th particle in generation $t$.

In conditional SMC we condition on the immortal line surviving each resampling step. By exchangeability we can set without loss of generality that the immortal line consists of particle 1 in each generation. At each resampling step, particle 1 must therefore choose particle 1 as its parent, while the remaining $N-1$ offspring are assigned multinomially to the $N$ possible parents. The marginal offspring distributions are then
\begin{align*}
& \vttilde{1} \eqdist 1 + \Bin(N-1, \wt{1}) \\
& \vttilde{i} \eqdist \Bin(N-1, \wt{i}), \qquad i=2,\dots,N
\end{align*}
where we write $\vttilde{i}$ to distinguish from $\vt{i}$. Similarly, any other quantities written with tilde are the conditional SMC analogues of the corresponding untilded quantities.
\myrule

In the following we will make extensive use of the formula for factorial moments of the multinomial distribution given in \citet[p.67]{mosimann1962}, which has the convenient form
\begin{equation*}
\E[(X_i)_a(X_j)_b] = (n)_{a+b}\, p_i^a p_j^b
\end{equation*}
where $(X_1,\dots X_k) \sim \Mn(n, \mathbf{p})$.
To take advantage of this convenient expression we will convert any regular moments appearing in the calculations into factorial moments, using the identities summarised in Table \ref{tab:fact_powers}.
%Let us denote the random variables
%\begin{align*}
%& X_1,\dots, X_N \sim \Mn(N, \mathbf{w}) \\
%& \tilde{X}_1,\dots, \tilde{X}_N \sim \Mn(N-1, \mathbf{w}) \\
%& \mathbf{w} := (\wt{1},\dots, \wt{N})
%\end{align*}
\myrule

\begin{table}
\centering
\begin{tabular}{r c l}
\hline
$x$ & $=$ & $(x)_1$ \\
$x^2$ & $=$ & $(x)_2 + (x)_1$ \\
$x^3$ & $=$ & $(x)_3 + 3(x)_2 + (x)_1$ \\
$x^4$ & $=$ & $(x)_4 + 6(x)_3 + 7(x)_2 + (x)_1$ \\
\hline
$xy$ & $=$ & $(x)_1(y)_1$ \\
$x^2y$ & $=$ & $(x)_2(y)_1 + (x)_1(y)_1$ \\
$xy^2$ & $=$ & $(x)_1(y)_2 + (x)_1(y)_1$ \\
$x^2y^2$ & $=$ & $(x)_2(y)_2 + (x)_2(y)_1 + (x)_1(y)_2 +  (x)_1(y)_1$ \\
\hline
\end{tabular}
\caption{Conversion of regular powers into falling factorial powers}
\label{tab:fact_powers}
\end{table}

The coalescence rate is given by
\begin{equation*}
c_N(t) := \frac{1}{(N)_2} \sum_{i=1}^{N} \E\left[ (\vt{i})_2 \right].
\end{equation*}
In the following, all expectations are conditional on $\F$ but the conditioning is omitted for notational brevity.\\
In the standard case we have, using the tower rule and the multinomial moments,
\begin{equation*}
\E[c_N(t) |\F] 
= \frac{1}{(N)_2} \sum_{i=1}^{N} \E\left[ \E[ (\vt{i})_2 ] |\F \right]
=\frac{1}{(N)_2} \sum_{i=1}^{N} \E\left[(N)_2 (\wt{i})^2 |\F \right] 
= \sum_{i=1}^{N} \E\left[(\wt{i})^2 |\F \right]
\end{equation*}
The corresponding quantity in the conditional case is
\begin{equation*}
\tilde{c}_N(t) = \frac{1}{(N)_2} \sum_{i=1}^{N} \E\left[ (\vttilde{i})_2 \right] 
= \frac{1}{(N)_2} \E\left[ (\vttilde{1})_2 \right] + \frac{1}{(N)_2}\sum_{i=2}^{N} \E\left[ (\vttilde{i})_2 \right] \\
\end{equation*}
Then applying the tower rule and factorial moments as before we obtain
\begin{align*}
\E[\tilde{c}_N(t) |\F] &= \frac{1}{(N)_2} \left\{ (N-1)_2\E[(\wt{1})^2 |\F] + 2(N-1)\E[\wt{1} |\F] + \sum_{i=2}^{N} (N-1)_2\E[(\wt{i})^2 |\F] \right\}\\
&= \frac{(N-1)_2}{(N)_2} \sum_{i=1}^{N} \E[(\wt{i})^2 |\F] + \frac{2(N-1)}{(N)_2} \E[\wt{1} |\F] \\
&= \frac{N-2}{N} \E[c_N(t) |\F] + \frac{2}{N} \E[\wt{1} |\F]
\end{align*}
\myrule

The rate of super-binary mergers is bounded above by
\begin{align*}
D_N(t) &:= \frac{1}{N(N)_2} \sum_{i=1}^N (\vt{i})_2 \left( \vt{i} + \frac{1}{N} \sum_{j\neq i} (\vt{j})^2 \right) \\
&= \frac{1}{N(N)_2} \sum_{i=1}^N \left\{ (\vt{i})_2\vt{i} + \frac{1}{N}\sum_{j\neq i} (\vt{i})_2(\vt{j})^2 \right\} \\
&= \frac{1}{N(N)_2} \sum_{i=1}^N \left\{ (\vt{i})_3 + 2(\vt{i})_2 + \frac{1}{N}\sum_{j\neq i} \left( (\vt{i})_2(\vt{j})_2 + (\vt{i})_2\vt{j} \right) \right\}
\end{align*}
We therefore find
\begin{align*}
\E[D_N(t) |\F] &= \frac{1}{N(N)_2} \sum_{i=1}^N \left\{ (N)_3\E[(\wt{i})^3 |\F] + 2(N)_2\E[(\wt{i})^2 |\F] \right\} \\
&\qquad + \frac{1}{N^2(N)_2} \sum_{i=1}^N \sum_{j\neq i} \left\{ (N)_4\E[(\wt{i})^2(\wt{j})^2 |\F] + (N)_3\E[(\wt{i})^2\wt{j} |\F] \right\}
\end{align*}
For the conditional case, we separate the terms involving particle 1:
\begin{align*}
\tilde{D}_N(t) &= \frac{1}{N(N)_2} (\vttilde{1})_2 \left(\vttilde{1} + \frac{1}{N} \sum_{j\neq 1} (\vttilde{j})^2 \right)
+ \frac{1}{N(N)_2} \sum_{i\neq 1} (\vttilde{i})_2 \left( \vttilde{i} + \frac{1}{N}(\vttilde{1})^2 + \frac{1}{N} \sum_{1\neq j\neq i} (\vttilde{j})^2 \right)\\
&= \frac{1}{N(N)_2} \left\{ (\vttilde{1})_2\vt{1} + \frac{1}{N}\sum_{j\neq 1} (\vttilde{1})_2(\vttilde{j})^2 + \frac{1}{N}\sum_{i\neq 1} (\vttilde{i})_2(\vttilde{1})^2 \right\} \\
&\qquad + \frac{1}{N(N)_2} \sum_{i\neq 1} \left\{ (\vttilde{i})_2\vttilde{i} + \frac{1}{N}\sum_{1\neq j\neq i} (\vttilde{i})_2(\vttilde{j})^2 \right\} \\
&= \frac{1}{N(N)_2} \left\{ (\vttilde{1})_2\vttilde{1} + \frac{1}{N}\sum_{i\neq 1} \left( (\vttilde{1})_2(\vttilde{i})^2 + (\vttilde{i})_2(\vttilde{1})^2 \right) \right\} \\
&\qquad + \frac{1}{N(N)_2} \sum_{i\neq 1} \left\{ (\vttilde{i})_3 + 2(\vttilde{i})_2 + \frac{1}{N}\sum_{1\neq j\neq i} \left( (\vttilde{i})_2(\vttilde{j})_2 + (\vttilde{i})_2\vttilde{j} \right) \right\}
\end{align*}
and we find the expectation..... (now collapse 1 terms into sum, then give bound in terms of E(D).) not convinced the next equality is right in book...
\begin{align*}
\E[\tilde{D}_N(t) |\F] &= \frac{1}{N(N)_2} \left\{ (N-1)_3\E[(\wt{1})^3 |\F] + 5(N-1)_2\E[(\wt{1})^2 |\F] +  4(N-1)\E[\wt{1} |\F] \right\} \\
&\qquad + \frac{1}{N^2(N)_2} \sum_{i=2}^N \left\{ 2(N-1)_4\E[(\wt{1})^2(\wt{i})^2 |\F] + 2(N-1)_3 \E[\wt{1}(\wt{i})^2 |\F] \right.\\
&\hspace{160pt} \left. -2(N-1)_2\E[\wt{1}\wt{i} |\F] + (N-1)_2\E[(\wt{i})^2 |\F] \right\} \\
&\qquad + \frac{1}{N(N)_2} \sum_{i=2}^N \left\{ 
(N-1)_3\E[(\wt{i})^3 |\F] + 2(N-1)_2\E[(\wt{i})^2 |\F] \right\} \\
&\qquad + \frac{1}{N^2(N)_2} \sum_{i=2}^N \sum_{1\neq j\neq i} \left\{ (N-1)_4\E[(\wt{i})^2(\wt{j})^2 |\F] + (N-1)_3\E[(\wt{i})^2\wt{j} |\F] \right\} \\
&=
\end{align*}
\myrule

We also require a bound on the squared coalescence rate, obtained by a similar separation of terms involving particle 1.
\begin{align*}
\E[c_N(t)^2] &= \frac{1}{(N)_2^2} \E\left[ \left( \sum_{i=1}^N (\vt{i})_2\right)^2\right] \\
&= \frac{1}{(N)_2^2} \left\{ \sum_{i=1}^N \E[(\vt{i})_2^2] + \sum_{i=1}^N\sum_{j\neq i} \E[(\vt{i})_2(\vt{j})_2] \right\} \\
&= \frac{1}{(N)_2^2} \left\{ (N)_4 \sum_{i=1}^N \E[(\wt{i})^4] + 4(N)_3 \sum_{i=1}^N \E[(\wt{i})^3] + 2(N)_2 \sum_{i=1}^N \E[(\wt{i})^2] \right\} \\
&\qquad + \frac{1}{(N)_2^2}(N)_4 \sum_{i=1}^N\sum_{j\neq i} \E[(\wt{i})^2(\wt{j})^2]
\end{align*}
and
\begin{align*}
\E[\tilde{c}_N(t)^2] 
&= \frac{1}{(N)_2^2} \left\{ \sum_{i=1}^N \E[(\vttilde{i})_2^2] 
+ \sum_{i=1}^N\sum_{j\neq i} \E[(\vttilde{i})_2(\vttilde{j})_2] \right\} \\
&= \frac{1}{(N)_2^2} \left\{ \sum_{i=2}^N \E[(\vttilde{i})_2^2] +\E[(\vttilde{1})_2^2] 
+ \sum_{i=2}^N\sum_{1\neq j\neq i} \E[(\vttilde{i})_2(\vttilde{j})_2] + 2\sum_{i=1}^N \E[(\vttilde{1})_2(\vttilde{i})_2] \right\} \\
&= \frac{1}{(N)_2^2} \left\{ (N-1)_4 \sum_{i=1}^N \E[(\wt{i})^4] + 4(N-1)_3 \sum_{i=1}^N \E[(\wt{i})^3] + 2(N-1)_2 \sum_{i=1}^N \E[(\wt{i})^2] \right\} \\
&\qquad + \frac{1}{(N)_2^2} \left\{ (N-1)_4 \sum_{i=1}^N\sum_{j\neq i} \E[(\wt{i})^2(\wt{j})^2] 
+ 4(N-1)_3 \E[(\wt{1})^3] + 12(N-1)_2\E[(\wt{1})^2] + 4(N-1)\E[\wt{1}] \right\} \\
&\qquad +\frac{1}{(N)_2^2} \left\{ 2(N-1)_3 \sum_{i=2}^N \E[\wt{1}(\wt{i})^2] \right\} \\
&\leq  \E[c_N(t)^2] + \frac{1}{(N)_2^2} \left\{4(N-1)_3 \E[(\wt{1})^3] + 12(N-1)_2\E[(\wt{1})^2] + 4(N-1)\E[\wt{1}]
+ 2(N-1)_3 \sum_{i=2}^N \E[\wt{1}(\wt{i})^2] \right\}
\end{align*}
\myrule

We can modify the proof of \citet[Lemma 3]{koskela2018} by replacing the untilded quantities with these modified expressions.
We still assume the conditions (18) and (19) of the lemma, which control the weights and allow us to find the following asymptotic expressions:
\begin{align*}
& \E[\tilde{c}_N(t)] \leq \E[c_N(t)] + O(N^{-2}) \\
& \E[\tilde{c}_N(t)^2] \leq \E[c_N(t)^2] + O(N^{-3}) \\
& \E[\tilde{D}_N(t)] \leq \E[D_N(t)] +O(N^{-3})
\end{align*}
Therefore we get the upper bound
\begin{align*}
\E[\tilde{c}_N(t)] &= \frac{N-2}{N} \E[c_N(t)] + \frac{2}{N} \E[\wt{1}] \\
& \leq \E[c_N(t)] + \frac{2}{N} \E[\wt{1}] \\
&\leq \frac{a^4}{N\varepsilon^4} +  \frac{2}{N} \E[\wt{1}] \\
&= \frac{a^4}{N\varepsilon^4} + O(N^{-2})
\end{align*}
and lower bound
\begin{align*}
\E[\tilde{c}_N(t)] &= \frac{N-2}{N} \E[c_N(t)] + \frac{2}{N} \E[\wt{1}] \\
& \geq  \frac{N-2}{N}\frac{\varepsilon^4}{Na^4} + O(N^{-2}) \\
& = \frac{\varepsilon^4}{Na^4} - \frac{2\varepsilon^4}{N^2a^4} + O(N^{-2}) \\
& = \frac{\varepsilon^4}{Na^4} + O(N^{-2})
\end{align*}
as in \citet{koskela2018}, with the addition of an error term.
Using that (22) in the lemma implies $\E[c_N(t)] = O(N^{-1})$, we also have the reverse inequality
\begin{equation*}
\E[\tilde{c}_N(t)] = \E[c_N(t)] - \frac{2}{N}\E[c_N(t)] + O(N^{-2}) \geq \E[c_N(t)] + O(N^{-2}).
\end{equation*}
This allows us to deduce the following upper bounds:
\begin{align*}
\E[\tilde{D}_N(t)] &\leq \E[D_N(t)] + O(N^{-3}) \\
&\leq \frac{C}{N} \E[c_N(t)] + O(N^{-3}) \\
&= \frac{C}{N} \E[\tilde{c}_N(t)] + O(N^{-3})
\end{align*}
and
\begin{align*}
\E[\tilde{c}_N(t)^2] &\leq \E[c_N(t)^2] + O(N^{-3}) \\
&\leq \frac{C}{N} \E[c_N(t)] + O(N^{-3}) \\
&= \frac{C}{N} \E[\tilde{c}_N(t)] + O(N^{-3})
\end{align*}
which correspond to (20) and (21) respectively in the statement of the lemma, again with additional error terms. $C$ indicates an arbitrary constant which may change from one line to the next.

We conclude an analogous result to \citet[Lemma 3]{koskela2018} for conditional SMC, involving the tilded quantities and with the addition of some vanishing errors that should not affect the further analysis.

\bibliography{smc.bib}
\end{document}