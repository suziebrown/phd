\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=4cm]{geometry}

\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{enumitem}

\renewcommand{\arraystretch}{1.4}
\usepackage{xspace}
\newcommand{\seb}[1]{\xspace\textcolor{red}{#1 ---SB}\xspace} %% for comments

%%% MATHS ENVIRONMENTS
\usepackage[nobreak]{mdframed}
\newmdtheoremenv[backgroundcolor=gray!20, linewidth=0pt]{theorem}{Theorem}
\newmdtheoremenv[backgroundcolor=gray!20, linewidth=0pt]{corollary}[theorem]{Corollary}
\newmdtheoremenv[backgroundcolor=gray!20, linewidth=0pt]{prop}[theorem]{Proposition}
\newmdtheoremenv[backgroundcolor=gray!20, linewidth=0pt]{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newmdtheoremenv[backgroundcolor=gray!20, linewidth=0pt]{defn}[theorem]{Definition}
\renewcommand{\qedsymbol}{$\blacksquare$}

%%% MATHS COMMANDS
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Et}{\mathbb{E}_t}
\newcommand{\V}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\ON}{1_N}
\newcommand{\eqdist}{\overset{d}{=}}
\newcommand{\I}[1]{\mathbbm{1}_{\{#1\}}}
\newcommand{\1}[1]{\mathbbm{1}_{#1}} % JK uses mathds{1} for indicators
\newcommand{\midd}{\,\middle|\,}        % for big conditional probability bar
\newcommand{\Mn}{\operatorname{Multinomial}}
\newcommand{\Bin}{\operatorname{Binomial}}
\newcommand{\Cat}{\operatorname{Categorical}}
\newcommand{\Exp}{\operatorname{Exp}}
\newcommand{\Unif}{\operatorname{Uniform}}
\newcommand{\Bern}{\operatorname{Bernoulli}}
\newcommand{\flnw}[1][i]{\lfloor N w_t^{(#1)} \rfloor}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

%%% GRAPHICS
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}
\usepackage{subfig}

% custom header/footer
\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\fancyhf{}
\rfoot{\textsf{\thepage}}
\lfoot{\textsf{Suzie Brown}}

%%% HYPERLINKS (must load last)
\usepackage{hyperref}

\title{Residual resampling with multinomial residuals}
\author{Suzie Brown}
\date{29 April 2021}

\begin{document}
\maketitle
\thispagestyle{fancy}

\begin{enumerate}[label=(A\arabic*)]
\item\label{standing_assumption} The conditional distribution of parental indices $a_t^{(1:N)}$ given offspring counts $\nu_t^{(1:N)}$ is uniform over all assignments such that $ |\{ j: a_t^{(j)} =i \}|= \nu_t^{(i)} $ for all $i$.
%valid assignments.
\end{enumerate}

\begin{corollary}\label{thm:residual_multinomial}
Consider an SMC algorithm using residual resampling with multinomial residuals, such that \ref{standing_assumption} is satisfied.
Assume that there exists a constant $a\in [1,\infty)$ such that for all $x, x^\prime, t$,
\begin{equation*}
\frac{1}{a} \leq g_t(x, x^\prime) \leq a .
\end{equation*}
Assume that $\Prob[ \tau_N(t) = \infty] =0$ for all finite $t$.
Let $(G_t^{(n,N)})_{t\geq0}$ denote the genealogy of a random sample of $n$ terminal particles from the output of the algorithm when the total number of particles used is $N$. Then, for any fixed $n$, the time-scaled genealogy $(G_{\tau_N(t)}^{(n,N)})_{t\geq0}$ converges to Kingman's $n$-coalescent as $N\to \infty$, in the sense of finite-dimensional distributions.
\end{corollary}

\begin{proof}
With residual-multinomial resampling, for each $i$
\begin{equation*}
\nu_t^{(i)} \mid w_t^{(1:N)}
\eqdist \flnw + X_i
\end{equation*}
where $X_i \sim \Bin(R, r_i)$. As usual, $R := N - \sum_{i=1}^N \flnw$ and $r_i := ( Nw_t^{(i)} - \flnw ) /R$.
\seb{If $R=0$ then $r_i = 0$ for all $i$ and the following calculations remain correct.}
We can therefore compute
\begin{align*}
\E [ (\nu_t^{(i)})_2 \mid w_t^{(1:N)} ]
&= \E\left[ (\flnw + X_i) (\flnw + X_i -1) \midd w_t^{(1:N)} \right] \\
&= (\flnw)_2 + 2\flnw E[ X_i \mid w_t^{(1:N)} ] + \E[ (X_i)_2 \mid w_t^{(1:N)} ] \\
&= (\flnw)_2 + 2\flnw R r_i + (R)_2 r_i^2
\end{align*}
using the moments of the Binomial distribution.
We also have
\begin{align*}
\E [ (\nu_t^{(i)})_3 \mid w_t^{(1:N)} ]
&= \E\left[ (\flnw + X_i) (\flnw + X_i -1) (\flnw + X_i -2) \midd w_t^{(1:N)} \right] \\
&= \flnw^3 + \flnw^2 \E[ 3X_i -3 \mid w_t^{(1:N)} ] \\
    &\hspace{1cm}+ \flnw 
        \E[ X_i(X_i-1) + X_i(X_i-2) + (X_i-1)(X_i-2) \mid w_t^{(1:N)} ] \\
    &\hspace{1cm}+ \E[ (X_i)_3 \mid w_t^{(1:N)} ] \\
&= \flnw^3 - 3\flnw^2 +3\flnw^2 \E[ X_i \mid w_t^{(1:N)} ] \\
    &\hspace{1cm}+ \flnw \E[ 3X_i^2 - 6X_i +2 \mid w_t^{(1:N)} ] 
        + E[(X_i)_3 \mid w_t^{(1:N)} ]\\
&= \left( \flnw^3 - 3\flnw^2 + 2\flnw \right)
        + 3 \left( \flnw^2 - \flnw \right) \E[ X_i \mid w_t^{(1:N)} ] \\
    &\hspace{1cm}+ 3 \flnw \E[ (X_i)_2 \mid w_t^{(1:N)} ] 
        + \E[ (X_i)_3 \mid w_t^{(1:N)} ] \\
&= (\flnw)_3 + 3(\flnw)_2 R r_i + 3\flnw (R)_2 r_i^2 + (R)_3 r_i^3 \\
&\leq \left( \flnw + R r_i \right) \left\{ (\flnw)_2 + 2\flnw R r_i 
        + (R)_2 r_i^2 \right\} \\
&= Nw_t^{(i)} \E[(\nu_t^{(i)})_2 \mid w_t^{(1:N)} ] \\
&\leq a^2 \E[(\nu_t^{(i)})_2 \mid w_t^{(1:N)} ] ,
\end{align*}
using the almost sure bound $w_t^{(i)} \leq a^2/N$.

...

\seb{
To complete the proof we need to exchange the conditioning on $w_t^{(1:N)}$ for conditioning on $\mathcal{H}_t$ so we can then invoke the D-separation and tower property to get
\begin{equation*}
\frac{1}{(N)_3} \sum_{i=1}^N \Et [ (\nu_t^{(i)})_3 ]
\leq b_N \frac{1}{(N)_2} \sum_{i=1}^N \Et [ (\nu_t^{(i)})_2 ] 
\end{equation*}
for some sequence $b_N \to 0$.
Perhaps we should expect bounds on $q_t$ to be required, so that $\varepsilon$ will also appear in $b_N$.
}

\end{proof}

\end{document}